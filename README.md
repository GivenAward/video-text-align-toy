# Videoâ€“Text Alignment Toy Pipeline

A toy project for building a **videoâ€“text aligned dataset** from YouTube videos, designed for Visual-Language Model (VLM) experiments.

This project focuses on **pipeline design, alignment logic, and data quality control**, rather than code optimization or model performance.

---

## ðŸŽ¯ Project Goal

The goal of this project is to automatically generate **(video clip, aligned text)** pairs that can be directly used for VLM training or prototyping.

Key objectives:
- Build an end-to-end multimodal data pipeline
- Align video clips with spoken text using timestamps
- Apply explicit filtering rules to improve data quality
- Clearly explain *why* each design decision was made

---

## ðŸ§© Pipeline Overview

The pipeline consists of several steps to convert raw YouTube videos into **aligned videoâ€“text pairs** suitable for VLM experiments:

```text
YouTube Videos (10)
      â†“
Audio Extraction
      â†“
Whisper ASR (Timestamped Transcript)
      â†“
Sentence-level Video Clip Segmentation
      â†“
Videoâ€“Text Alignment
      â†“
Filtering & Quality Control
      â†“
Final Dataset (clip.mp4, caption)
